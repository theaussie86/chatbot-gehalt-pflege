---
phase: 04-edge-function-processing
plan: 02
type: execute
wave: 2
depends_on: ["01"]
files_modified:
  - supabase/functions/process-embeddings/index.ts
  - supabase/migrations/add_processing_columns.sql
autonomous: true

must_haves:
  truths:
    - "Embedding API response is parsed defensively with multiple fallback paths"
    - "Batch embedding failures don't kill entire document processing"
    - "Document processing fails completely if any chunk fails (all-or-nothing per CONTEXT.md)"
    - "Status updates show processing stage (extracting text / embedding chunks)"
    - "Document records chunk count after successful embedding"
    - "processing_stage column exists in documents table"
  artifacts:
    - path: "supabase/functions/process-embeddings/index.ts"
      provides: "Defensive embedding parsing and improved batch handling"
      contains: "Promise.allSettled"
    - path: "supabase/migrations/add_processing_columns.sql"
      provides: "Migration for processing_stage and chunk_count columns"
      contains: "processing_stage"
  key_links:
    - from: "supabase/functions/process-embeddings/index.ts"
      to: "Gemini Embedding API"
      via: "genAI.models.embedContent"
      pattern: "embedding\\.values.*embeddings\\[0\\]\\.values"
    - from: "supabase/functions/process-embeddings/index.ts"
      to: "documents table"
      via: "supabase.update"
      pattern: "processing.*extracting|processing.*embedding"
---

<objective>
Implement defensive embedding response parsing and improve batch processing to handle partial failures gracefully.

Purpose: The embedding API response structure varies between SDK versions. Current code assumes `embeddings[0].values` but may receive `embedding.values`. Additionally, Promise.all cascades failures - one failed chunk kills the batch.

Output: Edge function with defensive response parsing and Promise.allSettled for batch processing, with all-or-nothing document failure semantics.
</objective>

<execution_context>
@/Users/cweissteiner/.claude/get-shit-done/workflows/execute-plan.md
@/Users/cweissteiner/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/STATE.md
@.planning/phases/04-edge-function-processing/04-CONTEXT.md
@.planning/research/PITFALLS.md

@supabase/functions/process-embeddings/index.ts
@apps/api/lib/vectorstore/VectorstoreService.ts
</context>

<tasks>

<task type="auto">
  <name>Task 1: Add migration for processing columns and implement defensive embedding parsing</name>
  <files>supabase/migrations/add_processing_columns.sql, supabase/functions/process-embeddings/index.ts</files>
  <action>
First, create the migration to add the required columns. Then implement defensive embedding parsing.

**Step 1: Create migration file**

Create `supabase/migrations/YYYYMMDDHHMMSS_add_processing_columns.sql` (use current timestamp):
```sql
-- Add processing_stage column to track where processing is
ALTER TABLE documents
ADD COLUMN IF NOT EXISTS processing_stage text;

-- Add chunk_count column to store number of chunks created
ALTER TABLE documents
ADD COLUMN IF NOT EXISTS chunk_count integer;

-- Comment for documentation
COMMENT ON COLUMN documents.processing_stage IS 'Current processing stage: extracting_text, embedding, inserting, or null when complete';
COMMENT ON COLUMN documents.chunk_count IS 'Number of chunks created after successful embedding';
```

**Step 2: Create extractEmbeddingValues helper function**

Add this helper function near the top of the edge function file (after imports):
```typescript
/**
 * Extract embedding values with multiple fallback paths for SDK version compatibility.
 * EDGE-01: Handles both v0.x format (embeddings[0].values) and v1.x format (embedding.values)
 */
function extractEmbeddingValues(embedResult: any): number[] | null {
  // Try multiple response formats (SDK version variations)
  const values =
    embedResult?.embedding?.values ||        // v1.x format (singular)
    embedResult?.embeddings?.[0]?.values ||  // v0.x format (array)
    embedResult?.values ||                    // Direct format
    null;

  // Validate the result
  if (!values || !Array.isArray(values) || values.length === 0) {
    console.error('Invalid embedding response structure:', JSON.stringify(embedResult, null, 2));
    return null;
  }

  // Validate dimension (text-embedding-004 should be 768)
  if (values.length !== 768) {
    console.warn(`Unexpected embedding dimension: ${values.length} (expected 768)`);
  }

  return values;
}
```

**Step 3: Replace current embedding extraction**

Find the current embedding extraction code (around line 142) and replace:
```typescript
// OLD code to find and replace:
const values = embedResult.embeddings?.[0]?.values;
if (!values) return null;

// NEW code:
const values = extractEmbeddingValues(embedResult);
if (!values) {
  throw new Error(`Failed to extract embedding for chunk ${i + batchIndex}: invalid response structure`);
}
```

**Step 4: Add progress logging**

Add logging before embedding each chunk:
```typescript
console.log(`Embedding chunk ${absoluteIndex + 1}/${chunks.length}`);
```
  </action>
  <verify>
- Confirm migration file exists with processing_stage and chunk_count columns
- Confirm `extractEmbeddingValues` helper function exists in edge function
- Confirm it checks `embedding.values`, `embeddings[0].values`, and `values` paths
- Confirm it validates array and non-empty
- Confirm it logs the full response on failure for debugging
  </verify>
  <done>
- Migration adds processing_stage and chunk_count columns
- Embedding response parsing uses multiple fallback paths
- Invalid responses are logged with full structure for debugging
- Dimension validation warns on unexpected sizes
  </done>
</task>

<task type="auto">
  <name>Task 2: Implement Promise.allSettled with all-or-nothing semantics</name>
  <files>supabase/functions/process-embeddings/index.ts</files>
  <action>
Implement Promise.allSettled for batch processing while maintaining all-or-nothing document failure semantics per CONTEXT.md.

**Step 1: Add status updates to show processing stage**

Before text extraction begins, update document status:
```typescript
currentStage = 'extracting_text';
await supabase
  .from("documents")
  .update({ status: "processing", processing_stage: "extracting text" })
  .eq("id", documentId);
```

Before embedding begins, update processing stage:
```typescript
currentStage = 'embedding';
await supabase
  .from("documents")
  .update({ processing_stage: "embedding chunks" })
  .eq("id", documentId);
```

**Step 2: Replace Promise.all with Promise.allSettled**

Find the batch processing loop and replace Promise.all with Promise.allSettled. The new implementation:

```typescript
// Build batch promises with index tracking for error reporting
const batchPromises = batch.map(async (chunkText, batchIndex) => {
  const absoluteIndex = i + batchIndex;
  try {
    console.log(`Embedding chunk ${absoluteIndex + 1}/${chunks.length}`);

    const embedResult = await genAI.models.embedContent({
      model: GEMINI_MODEL_EMBED,
      contents: chunkText,
    });

    const values = extractEmbeddingValues(embedResult);
    if (!values) {
      return null; // Will be caught as failure below
    }

    return {
      document_id: documentId,
      chunk_index: absoluteIndex,
      content: chunkText,
      embedding: values,
      token_count: Math.ceil(chunkText.length / 4)
    };
  } catch (e) {
    console.error(`Failed to embed chunk ${absoluteIndex}:`, e);
    const error = e as Error & { index?: number };
    error.index = absoluteIndex; // Attach index for error reporting
    throw error;
  }
});

// Use Promise.allSettled instead of Promise.all
const batchResults = await Promise.allSettled(batchPromises);

// Separate successful and failed results
const successful: typeof chunkDataArray = [];
const failed: { index: number; error: string }[] = [];

for (const result of batchResults) {
  if (result.status === 'fulfilled' && result.value !== null) {
    successful.push(result.value);
  } else if (result.status === 'rejected') {
    const reason = result.reason as Error & { index?: number };
    failed.push({
      index: reason?.index ?? -1,
      error: reason?.message ?? 'Unknown error'
    });
  } else if (result.status === 'fulfilled' && result.value === null) {
    // extractEmbeddingValues returned null (already logged)
    failed.push({
      index: -1,
      error: 'Invalid embedding response structure'
    });
  }
}

// ALL-OR-NOTHING: If ANY chunk fails, fail the entire document
if (failed.length > 0) {
  throw new Error(
    `Embedding failed for ${failed.length} of ${batch.length} chunks in batch. ` +
    `First failure: ${failed[0].error}`
  );
}

chunkDataArray.push(...successful);
```

**Step 3: Add chunk count to final status update**

After successfully inserting all chunks, update the document with chunk count:
```typescript
await supabase
  .from("documents")
  .update({
    status: "embedded",
    chunk_count: chunkDataArray.length,
    processing_stage: null // Clear processing stage on completion
  })
  .eq("id", documentId);
```
  </action>
  <verify>
- Confirm Promise.allSettled is used (not Promise.all)
- Confirm batchResults are separated into successful and failed arrays
- Confirm failed chunks cause entire document to fail (all-or-nothing check)
- Confirm status updates show "extracting text" and "embedding chunks" stages
- Confirm chunk_count is stored on successful completion
- Confirm processing_stage is set to null on completion
  </verify>
  <done>
- Promise.allSettled used for batch processing
- All-or-nothing semantics: any chunk failure fails the document
- Processing stage visible in status updates
- Chunk count stored in documents table on success
- Processing stage cleared on completion
  </done>
</task>

</tasks>

<verification>
After both tasks complete:

1. **Migration check**: Verify migration file exists and has correct SQL

2. **Code review**: Verify the edge function has:
   - `extractEmbeddingValues` helper with 3 fallback paths
   - `Promise.allSettled` (not `Promise.all`)
   - All-or-nothing failure check (if failed.length > 0, throw)
   - Status updates with processing_stage field

3. **Pattern check**: The batch processing should follow:
```
for each batch:
  Promise.allSettled(batchPromises)
  separate successful vs failed
  if any failed -> throw Error (all-or-nothing)
  accumulate successful results
```

4. **Verify status flow**: processing (extracting text) -> processing (embedding chunks) -> embedded/error
</verification>

<success_criteria>
- Migration creates processing_stage and chunk_count columns
- Embedding response parsing handles v0.x and v1.x SDK formats
- Promise.allSettled used instead of Promise.all
- Any chunk failure fails the entire document (all-or-nothing)
- Processing stage shown during execution (visible in realtime UI)
- Chunk count stored on successful completion
</success_criteria>

<output>
After completion, create `.planning/phases/04-edge-function-processing/04-02-SUMMARY.md`
</output>

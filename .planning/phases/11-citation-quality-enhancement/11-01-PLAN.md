---
phase: 11-citation-quality-enhancement
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - apps/api/migrations/20260203000000_add_page_data_to_chunks.sql
  - apps/api/lib/inngest/functions/process-document.ts
autonomous: true

must_haves:
  truths:
    - "Document chunks store page start and end numbers after processing"
    - "Documents are flagged when page extraction fails"
    - "SQL function returns page data with metadata queries"
  artifacts:
    - path: "apps/api/migrations/20260203000000_add_page_data_to_chunks.sql"
      provides: "Schema migration adding page_start, page_end to chunks, has_page_data to documents"
      contains: "page_start"
    - path: "apps/api/lib/inngest/functions/process-document.ts"
      provides: "Page-aware text extraction and chunk storage"
      contains: "page_start"
  key_links:
    - from: "apps/api/lib/inngest/functions/process-document.ts"
      to: "document_chunks"
      via: "Supabase insert with page data"
      pattern: "page_start.*page_end"
---

<objective>
Add page number tracking to document processing pipeline

Purpose: Enable citation quality by capturing page boundaries during text extraction
Output: Schema migration and updated processing function that extracts and stores page numbers
</objective>

<execution_context>
@/Users/cweissteiner/.claude/get-shit-done/workflows/execute-plan.md
@/Users/cweissteiner/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/phases/11-citation-quality-enhancement/11-CONTEXT.md

# Key files to modify
@apps/api/lib/inngest/functions/process-document.ts
@apps/api/migrations/20260125000000_match_documents_with_metadata.sql
</context>

<tasks>

<task type="auto">
  <name>Task 1: Add page data columns to database schema</name>
  <files>apps/api/migrations/20260203000000_add_page_data_to_chunks.sql</files>
  <action>
Create a migration file that:

1. Add columns to `document_chunks` table:
   - `page_start INTEGER` - Starting page number (nullable, null means unknown)
   - `page_end INTEGER` - Ending page number (nullable, null or same as start for single page)

2. Add column to `documents` table:
   - `has_page_data BOOLEAN DEFAULT NULL` - Flag indicating if page extraction succeeded
     - `NULL` = not yet processed with page extraction
     - `TRUE` = page data successfully extracted
     - `FALSE` = page extraction failed (fallback to chunk-only mode)

3. Update `match_documents_with_metadata` function to return page data:
   - Add `page_start` and `page_end` to RETURNS TABLE
   - Include in SELECT from document_chunks

4. Add comments documenting the columns

Migration should be idempotent (use IF NOT EXISTS patterns).
  </action>
  <verify>
Run: `cat apps/api/migrations/20260203000000_add_page_data_to_chunks.sql`
Verify: Contains ALTER TABLE for page_start, page_end, has_page_data columns
Verify: Contains updated match_documents_with_metadata function with page columns
  </verify>
  <done>
Migration file exists with:
- page_start INTEGER column on document_chunks
- page_end INTEGER column on document_chunks
- has_page_data BOOLEAN column on documents
- Updated SQL function returning page data
  </done>
</task>

<task type="auto">
  <name>Task 2: Extend Gemini extraction to capture page markers</name>
  <files>apps/api/lib/inngest/functions/process-document.ts</files>
  <action>
Modify the document processing Inngest function to extract page numbers:

1. **Update extraction prompt** in `getExtractionPrompt()`:
   - For PDFs, request page markers in format: `[PAGE:N]` before each page's content
   - Example prompt addition: "For PDF documents, prefix each page's content with [PAGE:N] where N is the page number. Start with [PAGE:1] for the first page."
   - Keep existing behavior for non-PDF types (spreadsheets, text files don't have pages)

2. **Add page parsing helper function**:
   ```typescript
   interface PagedContent {
     pageNumber: number;
     content: string;
   }

   function parsePageMarkers(text: string): PagedContent[] {
     // Split by [PAGE:N] markers
     // Return array of {pageNumber, content} objects
     // If no markers found, return single entry with pageNumber: null
   }
   ```

3. **Modify chunk-text step** to track page boundaries:
   - Parse extracted text into paged content
   - When splitting chunks, track which page(s) each chunk spans
   - Store page_start and page_end for each chunk

4. **Update chunk data structure** in generate-embeddings step:
   - Add page_start and page_end to chunk data object
   - Handle null values gracefully (chunks without page info)

5. **Update insert-chunks step**:
   - Include page_start and page_end in Supabase insert
   - After successful processing, update document with has_page_data flag:
     - Set `has_page_data = true` if any chunk has page data
     - Set `has_page_data = false` if no chunks have page data

6. **Error handling**:
   - If page marker parsing fails, log warning and continue without page data
   - Set has_page_data = false on the document
   - Do NOT fail the entire processing - pages are enhancement, not requirement

IMPORTANT:
- Maintain backward compatibility - existing documents without page data should continue to work
- The Inngest function must remain idempotent and resumable
- Keep the 2000 char chunk size and 100 char overlap from existing config
  </action>
  <verify>
Run: `grep -n "PAGE:" apps/api/lib/inngest/functions/process-document.ts`
Verify: Extraction prompt requests page markers for PDFs

Run: `grep -n "page_start\|page_end\|has_page_data" apps/api/lib/inngest/functions/process-document.ts`
Verify: Page fields referenced in chunk data and insert operations

Run: `npm run build --prefix apps/api`
Verify: TypeScript compiles without errors
  </verify>
  <done>
Document processing function:
- Requests page markers in extraction prompt for PDFs
- Parses [PAGE:N] markers from extracted text
- Tracks page boundaries during chunking
- Stores page_start and page_end for each chunk
- Sets has_page_data flag on document after processing
- Gracefully handles missing page data
  </done>
</task>

</tasks>

<verification>
After both tasks complete:

1. Schema verification:
   - Run migration in Supabase SQL Editor
   - Verify columns exist: `SELECT column_name FROM information_schema.columns WHERE table_name = 'document_chunks' AND column_name IN ('page_start', 'page_end');`

2. Processing verification:
   - Reprocess an existing PDF document from admin dashboard
   - Check document_chunks table for page_start and page_end values
   - Verify has_page_data flag is set on document

3. Function verification:
   - Test match_documents_with_metadata returns page columns
</verification>

<success_criteria>
- [ ] Migration file creates page_start, page_end columns on document_chunks
- [ ] Migration file creates has_page_data column on documents
- [ ] Migration file updates match_documents_with_metadata to return page data
- [ ] Process-document function extracts page markers from PDFs
- [ ] Process-document function stores page boundaries with chunks
- [ ] Process-document function sets has_page_data flag after processing
- [ ] TypeScript builds without errors
</success_criteria>

<output>
After completion, create `.planning/phases/11-citation-quality-enhancement/11-01-SUMMARY.md`
</output>
